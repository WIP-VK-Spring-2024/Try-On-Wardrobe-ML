{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'app' from '/usr/src/app/app/__init__.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "#sys.path.insert(0, \"/usr/src/app/app\")\n",
    "def import_path(fullpath):\n",
    "    \"\"\" \n",
    "    Import a file with full path specification. Allows one to\n",
    "    import from anywhere, something __import__ does not do. \n",
    "    \"\"\"\n",
    "    path, filename = os.path.split(fullpath)\n",
    "    filename, ext = os.path.splitext(filename)\n",
    "    sys.path.append(path)\n",
    "    module = __import__(filename)\n",
    "   # reload(module) # Might be out of date\n",
    "    del sys.path[-1]\n",
    "    return module\n",
    "\n",
    "import_path(\"/usr/src/app/app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from app.pkg.ml.try_on.preprocessing.aggregator import ClothProcessor\n",
    "cp = ClothProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from app.pkg.ml.auto_clothing_set.autoset import LocalRecSys\n",
    "\n",
    "from app.pkg.ml.buffer_converters import BytesConverter\n",
    "from PIL import Image\n",
    "from app.pkg.models.app.image_category import ImageCategory\n",
    "import random\n",
    "bc = BytesConverter()\n",
    "\n",
    "\n",
    "DATA_PATH = \"/usr/src/app/data\"\n",
    "def read_clothes(sub_folder):\n",
    "    \n",
    "    images_folder = os.path.join(DATA_PATH, sub_folder)\n",
    "    assert os.path.exists(images_folder)\n",
    "    im_paths = os.listdir(images_folder)\n",
    "    abs_im_path = [os.path.join(images_folder, im_path) for im_path in im_paths]\n",
    "    return abs_im_path\n",
    "\n",
    "\n",
    "outerwear_clothes_paths = read_clothes(\"outerwear\") # [\"/usr/src/app/data/upper/b_bloose.png\", \"/usr/src/app/data/upper/b_jacket.png\"]\n",
    "upper_clothes_paths = read_clothes(\"upper\") # [ \"/usr/src/app/data/upper/t-shirt-blue.png\",\"/usr/src/app/data/upper/t-shirt-miami.png\", \"/usr/src/app/data/upper/b_t-shirt-2.png\", \"/usr/src/app/data/upper/b_t-shirt-3.png\", \"/usr/src/app/data/upper/t-shirt-dc.png\", \"/usr/src/app/data/upper/t-shirt-dc2.png\", \"/usr/src/app/data/upper/t-shirt.png\" ]\n",
    "lower_clothes_paths = read_clothes(\"lower\")  # [\"/usr/src/app/data/lower/b_black_jeans.png\", \"/usr/src/app/data/lower/b_shorts.png\", \"/usr/src/app/data/lower/jeans-dc.png\", \"/usr/src/app/data/lower/shorts-dc.png\" ]\n",
    "dress_clothes_paths = read_clothes(\"dresses\") # [\"/usr/src/app/data/dresses/1.png\", \"/usr/src/app/data/dresses/2.png\", \"/usr/src/app/data/dresses/dress-dc.png\"]\n",
    "\n",
    "user_images = [f\"{DATA_PATH}/human/brayan_krenston.png\"]\n",
    "\n",
    "def convert_cloth_to_list(x, category=None):\n",
    "    res = []\n",
    "    for i in x:\n",
    "        image = Image.open(i)\n",
    "        im_no_back = cp.model_background(image)\n",
    "        im_bytes = bc.image_to_bytes(im_no_back)\n",
    "        res.append(image)\n",
    "    return res\n",
    "\n",
    "def convert_user_to_dict(x):\n",
    "    res = []\n",
    "    for i in x:\n",
    "        image = Image.open(i)\n",
    "        im_no_back = cp.model_background(image)\n",
    "        im_bytes = bc.image_to_bytes(im_no_back)\n",
    "        res.append({'image':im_bytes,})\n",
    "    return res\n",
    "\n",
    "#        im_white_back = cp.model_background.replace_background_RGBA(im_no_back)\n",
    "\n",
    "\n",
    "upper_clothes = convert_cloth_to_list(upper_clothes_paths, ImageCategory.UPPER_BODY)\n",
    "lower_clothes = convert_cloth_to_list(lower_clothes_paths, ImageCategory.LOWER_BODY)\n",
    "dress_clothes = convert_cloth_to_list(dress_clothes_paths, ImageCategory.DRESSES)\n",
    "outerwear_clothes = convert_cloth_to_list(outerwear_clothes_paths, ImageCategory.UPPER_BODY)\n",
    "user_images = convert_user_to_dict(user_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import SamModel, SamProcessor\n",
    "\n",
    "model = SamModel.from_pretrained(\"nielsr/slimsam-77-uniform\").to(\"cuda:0\")\n",
    "processor = SamProcessor.from_pretrained(\"nielsr/slimsam-77-uniform\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_url = \"https://huggingface.co/ybelkada/segment-anything/resolve/main/assets/car.png\"\n",
    "raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n",
    "input_points = [[[450, 600]]] # 2D localization of a window\n",
    "\n",
    "def get_im_with_mask()\n",
    "    inputs = processor(raw_image, input_points=input_points, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "    masks = processor.image_processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu())\n",
    "    scores = outputs.iou_scores\n",
    "    from PIL import Image\n",
    "    import torch\n",
    "    import torchvision.transforms as transforms\n",
    "\n",
    "    # Загрузка изображения с помощью PIL\n",
    "    image = raw_image\n",
    "\n",
    "    # Преобразование изображения PIL в тензор PyTorch\n",
    "    transform = transforms.ToTensor()\n",
    "    tensor_image = transform(image)\n",
    "\n",
    "    # Создание маски в формате torch.Tensor\n",
    "    #mask = torch.ones_like(tensor_image)[:, :, 0]  # Создание маски для канала красного\n",
    "    print(masks[0].shape)\n",
    "    mask = masks[0][0]#.unsqueeze(2)  # Добавление размерности для канала\n",
    "\n",
    "    # Применение маски к изображению\n",
    "    masked_image = tensor_image \n",
    "    #+ mask * 0.5  # Умножение на 0.5 для получения полупрозрачности\n",
    "    masked_image[mask] = 255\n",
    "    # Преобразование обратно в изображение PIL\n",
    "    masked_image = transforms.ToPILImage()(masked_image)\n",
    "\n",
    "    # Сохранение результирующего изображения\n",
    "    masked_image#.save('masked_image.jpg')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
