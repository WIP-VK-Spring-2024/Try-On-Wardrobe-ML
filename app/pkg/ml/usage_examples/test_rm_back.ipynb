{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'app' from '/usr/src/app/app/__init__.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "#sys.path.insert(0, \"/usr/src/app/app\")\n",
    "def import_path(fullpath):\n",
    "    \"\"\" \n",
    "    Import a file with full path specification. Allows one to\n",
    "    import from anywhere, something __import__ does not do. \n",
    "    \"\"\"\n",
    "    path, filename = os.path.split(fullpath)\n",
    "    filename, ext = os.path.splitext(filename)\n",
    "    sys.path.append(path)\n",
    "    module = __import__(filename)\n",
    "   # reload(module) # Might be out of date\n",
    "    del sys.path[-1]\n",
    "    return module\n",
    "\n",
    "import_path(\"/usr/src/app/app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from app.pkg.ml.try_on.preprocessing.aggregator import ClothProcessor\n",
    "cp = ClothProcessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "image = Image.open(\"/usr/src/app/data/etc/1.png\")\n",
    "im_no_back = cp.model_background(image)\n",
    "im_no_back.save(\"/usr/src/app/data/etc/1__.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from app.pkg.ml.auto_clothing_set.autoset import LocalRecSys\n",
    "\n",
    "from app.pkg.ml.buffer_converters import BytesConverter\n",
    "from PIL import Image\n",
    "from app.pkg.models.app.image_category import ImageCategory\n",
    "import random\n",
    "bc = BytesConverter()\n",
    "\n",
    "\n",
    "DATA_PATH = \"/usr/src/app/data\"\n",
    "def read_clothes(sub_folder):\n",
    "    \n",
    "    images_folder = os.path.join(DATA_PATH, sub_folder)\n",
    "    assert os.path.exists(images_folder)\n",
    "    im_paths = os.listdir(images_folder)\n",
    "    abs_im_path = [os.path.join(images_folder, im_path) for im_path in im_paths]\n",
    "    return abs_im_path\n",
    "\n",
    "\n",
    "outerwear_clothes_paths = read_clothes(\"outerwear\") \n",
    "upper_clothes_paths = read_clothes(\"upper\") \n",
    "lower_clothes_paths = read_clothes(\"lower\") \n",
    "dress_clothes_paths = read_clothes(\"dresses\")\n",
    "\n",
    "user_images = [f\"{DATA_PATH}/human/brayan_krenston.png\"]\n",
    "\n",
    "def convert_cloth_to_list(x, category=None):\n",
    "    res = []\n",
    "    for i in x:\n",
    "        image = Image.open(i)\n",
    "        im_no_back = cp.model_background(image)\n",
    "        im_bytes = bc.image_to_bytes(im_no_back)\n",
    "        res.append(image)\n",
    "    return res\n",
    "\n",
    "def convert_user_to_dict(x):\n",
    "    res = []\n",
    "    for i in x:\n",
    "        image = Image.open(i)\n",
    "        im_no_back = cp.model_background(image)\n",
    "        im_bytes = bc.image_to_bytes(im_no_back)\n",
    "        res.append({'image':im_bytes,})\n",
    "    return res\n",
    "\n",
    "#        im_white_back = cp.model_background.replace_background_RGBA(im_no_back)\n",
    "\n",
    "\n",
    "upper_clothes = convert_cloth_to_list(upper_clothes_paths, ImageCategory.UPPER_BODY)\n",
    "lower_clothes = convert_cloth_to_list(lower_clothes_paths, ImageCategory.LOWER_BODY)\n",
    "dress_clothes = convert_cloth_to_list(dress_clothes_paths, ImageCategory.DRESSES)\n",
    "outerwear_clothes = convert_cloth_to_list(outerwear_clothes_paths, ImageCategory.UPPER_BODY)\n",
    "user_images = convert_user_to_dict(user_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import SamModel, SamProcessor\n",
    "from transformers import SegformerImageProcessor, AutoModelForSemanticSegmentation\n",
    "\n",
    "# model = SamModel.from_pretrained(\"nielsr/slimsam-77-uniform\").to(\"cuda:0\")\n",
    "# processor = SamProcessor.from_pretrained(\"nielsr/slimsam-77-uniform\")\n",
    "\n",
    "processor = SegformerImageProcessor.from_pretrained(\"sayeed99/segformer_b3_clothes\")\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(\"sayeed99/segformer_b3_clothes\").to(\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "def change_contrast(img, level):\n",
    "    factor = (259 * (level + 255)) / (255 * (259 - level))\n",
    "    def contrast(c):\n",
    "        return 128 + factor * (c - 128)\n",
    "    return img.point(contrast)\n",
    "\n",
    "def get_im_with_mask(pil_im:Image.Image):\n",
    "        # new_contr = change_contrast(pil_im.convert('RGB'), 50)\n",
    "        image = np.array(pil_im.convert('RGB'))\n",
    "\n",
    "#        print(image.shape)\n",
    "        # convert('RGB') is for images with h,w,4 shape\n",
    "        \n",
    "\n",
    "        inputs = processor(image, return_tensors=\"pt\").to(\"cuda\")\n",
    "        with torch.no_grad(): \n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits.cpu()\n",
    "\n",
    "            upsampled_logits = nn.functional.interpolate(\n",
    "                logits,\n",
    "                size=pil_im.size[::-1],\n",
    "                mode=\"bilinear\",\n",
    "                align_corners=False,\n",
    "            )\n",
    "\n",
    "            pred_seg = upsampled_logits.argmax(dim=1)[0]\n",
    "\n",
    "            cloth_mask = pred_seg!=0\n",
    "            #masks = processor.image_processor.post_process_masks(outputs.pred_masks.cpu(), inputs[\"original_sizes\"].cpu(), inputs[\"reshaped_input_sizes\"].cpu())\n",
    "            \n",
    "            del inputs\n",
    "            del outputs\n",
    "\n",
    "        mask = cloth_mask\n",
    "\n",
    "        pil_im = Image.fromarray(mask.numpy())\n",
    "\n",
    "        # if background_color:\n",
    "        #     no_bg_image = Image.new(\"RGB\", pil_im.size, background_color)\n",
    "            \n",
    "        \n",
    "        no_bg_image = Image.new(\"RGBA\", pil_im.size, (0, 0 ,0 ,0))\n",
    "\n",
    "        orig_image = Image.fromarray(image[:,:,:])\n",
    "\n",
    "        no_bg_image.paste(orig_image, mask=pil_im)\n",
    "        result = no_bg_image\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uppers_no_back = [get_im_with_mask(im) for im in upper_clothes ]\n",
    "# lowers_no_back = [get_im_with_mask(im) for im in lower_clothes ]\n",
    "# outerwear_no_back = [get_im_with_mask(im) for im in outerwear_clothes ]\n",
    "# dresses_no_back = [get_im_with_mask(im) for im in dress_clothes ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in [*uppers_no_back, *lowers_no_back, *outerwear_no_back, *dresses_no_back]:\n",
    "#     i.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
